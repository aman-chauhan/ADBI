{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression with L2-Regularization (Ridge Regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements\n",
    "\n",
    "Libraries needed\n",
    "- numpy\n",
    "- scikit-learn (for testing the algorithm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD_Optimizer:\n",
    "    def __init__(self, batch_size, epochs, epsilon, learning_rate, regularization):\n",
    "        '''\n",
    "        Initializes the class with the configuration parameters. Also sets up\n",
    "        a loss history list and weight attribute.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_size: int\n",
    "            the size of batch to iterate over the dataset\n",
    "        epochs: int\n",
    "            the maximum number of epochs to run the Gradient Descent Algorithm\n",
    "        epsilon: float\n",
    "            the minimum update required to proceed to next epoch\n",
    "        learning_rate: float\n",
    "            the learning rate for Gradient Descent\n",
    "        regularization: float\n",
    "            the regularization strength for L2-Regularization\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        '''\n",
    "        self.epochs = epochs\n",
    "        self.epsilon = epsilon\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.regularization = regularization\n",
    "        self.loss = []\n",
    "        self.W = None\n",
    "        \n",
    "    def fit(self, X, Y):\n",
    "        '''\n",
    "        Function call to update the weights using Stochastic Gradient Descent\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X: numpy array or list\n",
    "            the features on which to learn the weights\n",
    "        Y: numpy array or list\n",
    "            the target values\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        '''\n",
    "        \n",
    "        # utility function to calculate Mean Squared Error\n",
    "        def MSE(y_true, y_pred):\n",
    "            return np.mean(np.square(y_true - y_pred))\n",
    "        \n",
    "        # change to numpy array if list\n",
    "        if isinstance(X, list):\n",
    "            X = np.asarray(X)\n",
    "        if isinstance(Y, list):\n",
    "            Y = np.asarray(Y)\n",
    "        \n",
    "        # if only 1-D array, change to 2-D array\n",
    "        if len(X.shape)==1:\n",
    "            X = np.expand_dims(X, -1)\n",
    "        # append a column of 1 for bias term\n",
    "        X = np.insert(X, 0, 1, axis=1)\n",
    "        \n",
    "        # weights to learn, initialized randomly\n",
    "        self.W = np.random.randn(X.shape[1])\n",
    "        \n",
    "        # indexes for quick shuffling of the X, Y arrays\n",
    "        indexes = np.asarray([i for i in range(X.shape[0])])\n",
    "        \n",
    "        # iterating for epochs\n",
    "        for _ in range(self.epochs):\n",
    "            # shuffle the indexes\n",
    "            np.random.shuffle(indexes)\n",
    "            \n",
    "            # track the loss for each batch iteration over X, Y\n",
    "            epochLoss = []\n",
    "            \n",
    "            # batchwise iteration\n",
    "            for i in range(0, X.shape[0], self.batch_size):\n",
    "                # extract the batches\n",
    "                batchX = X[indexes][i:min(i+self.batch_size, X.shape[0])]\n",
    "                batchY = Y[indexes][i:min(i+self.batch_size, X.shape[0])]\n",
    "                \n",
    "                # predictions using the batch of X\n",
    "                preds = np.matmul(batchX, self.W)\n",
    "                \n",
    "                # error associated with this batch\n",
    "                error = preds - batchY\n",
    "                \n",
    "                # append the loss to loss tracking of this epoch\n",
    "                epochLoss.append(MSE(batchY, preds) + self.regularization*np.sum(np.square(self.W)))\n",
    "                \n",
    "                # calculate gradient with L2-regularization\n",
    "                gradient = (np.matmul(batchX.T, error)/batchX.shape[0]) + self.regularization*self.W\n",
    "                \n",
    "                # update weights\n",
    "                self.W += -1*self.learning_rate*gradient\n",
    "            \n",
    "            # append mean loss of this epoch to loss history\n",
    "            self.loss.append(np.mean(epochLoss))\n",
    "            \n",
    "            # break if the last update was not significant\n",
    "            if len(self.loss) > 1 and abs(self.loss[-1] - self.loss[-2]) < self.epsilon:\n",
    "                break\n",
    "                \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Function to predict the target values using features provided\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X: numpy array or list\n",
    "            the features on which to predict the target values\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        numpy.ndarray\n",
    "            returns the predicted values\n",
    "        '''\n",
    "        # change to numpy array if list\n",
    "        if isinstance(X, list):\n",
    "            X = np.asarray(X)\n",
    "        \n",
    "        # if only 1-D array, change to 2-D array\n",
    "        if len(X.shape)==1:\n",
    "            X = np.expand_dims(X, -1)\n",
    "        \n",
    "        # append a column of 1 for bias term\n",
    "        X = np.insert(X, 0, 1, axis=1)\n",
    "        return np.matmul(X, self.W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution Example using Boston Housing Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_boston"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Boston Housing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = load_boston(return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into train and test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the Stochastic Gradient Descent Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = SGD_Optimizer(batch_size=10, epochs=1000, epsilon=0.01, learning_rate=0.001, regularization=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize the dataset using train mean and standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_min = X_train.min(0)\n",
    "train_max = X_train.max(0)\n",
    "X_train = (X_train - train_min)/(train_max - train_min)\n",
    "X_test = (X_test - train_min)/(train_max - train_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the optimizer on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loss Progression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch\tLoss\n",
      "0:\t485.7087\n",
      "1:\t399.1401\n",
      "2:\t340.7033\n",
      "3:\t283.8458\n",
      "4:\t240.5925\n",
      "5:\t213.5142\n",
      "6:\t187.2385\n",
      "7:\t170.3185\n",
      "8:\t158.6280\n",
      "9:\t147.0011\n",
      "10:\t135.8496\n",
      "11:\t134.5673\n",
      "12:\t122.1231\n",
      "13:\t119.3382\n",
      "14:\t118.8978\n",
      "15:\t113.9550\n",
      "16:\t107.6603\n",
      "17:\t108.1371\n",
      "18:\t107.0517\n",
      "19:\t103.4906\n",
      "20:\t101.2914\n",
      "21:\t102.5263\n",
      "22:\t98.5510\n",
      "23:\t97.4385\n",
      "24:\t96.1608\n",
      "25:\t95.7884\n",
      "26:\t99.2636\n",
      "27:\t94.0721\n",
      "28:\t96.2951\n",
      "29:\t91.3991\n",
      "30:\t90.7666\n",
      "31:\t90.1548\n",
      "32:\t91.8037\n",
      "33:\t89.0630\n",
      "34:\t89.7787\n",
      "35:\t87.2864\n",
      "36:\t86.6605\n",
      "37:\t86.8719\n",
      "38:\t87.6939\n",
      "39:\t84.5699\n",
      "40:\t83.9814\n",
      "41:\t85.0006\n",
      "42:\t82.9549\n",
      "43:\t84.8624\n",
      "44:\t81.5445\n",
      "45:\t80.7272\n",
      "46:\t80.8607\n",
      "47:\t83.4091\n",
      "48:\t85.9534\n",
      "49:\t79.2474\n",
      "50:\t79.3939\n",
      "51:\t77.7387\n",
      "52:\t78.1295\n",
      "53:\t76.8117\n",
      "54:\t76.9142\n",
      "55:\t75.7832\n",
      "56:\t75.5087\n",
      "57:\t74.9654\n",
      "58:\t78.4401\n",
      "59:\t76.2479\n",
      "60:\t73.5005\n",
      "61:\t73.6442\n",
      "62:\t73.2269\n",
      "63:\t72.6652\n",
      "64:\t72.4734\n",
      "65:\t72.7348\n",
      "66:\t72.0700\n",
      "67:\t71.6179\n",
      "68:\t74.0642\n",
      "69:\t73.0508\n",
      "70:\t70.8317\n",
      "71:\t70.0466\n",
      "72:\t72.9540\n",
      "73:\t69.5749\n",
      "74:\t69.5369\n",
      "75:\t71.0103\n",
      "76:\t68.0934\n",
      "77:\t69.0146\n",
      "78:\t70.0411\n",
      "79:\t67.3681\n",
      "80:\t67.0520\n",
      "81:\t68.2850\n",
      "82:\t66.8838\n",
      "83:\t65.9103\n",
      "84:\t66.6079\n",
      "85:\t65.4515\n",
      "86:\t65.7233\n",
      "87:\t65.5300\n",
      "88:\t65.2392\n",
      "89:\t64.4450\n",
      "90:\t64.5121\n",
      "91:\t66.1284\n",
      "92:\t64.3988\n",
      "93:\t63.9078\n",
      "94:\t63.4103\n",
      "95:\t63.5638\n",
      "96:\t63.3380\n",
      "97:\t63.5985\n",
      "98:\t62.9268\n",
      "99:\t64.8673\n",
      "100:\t63.1855\n",
      "101:\t65.6999\n",
      "102:\t62.1489\n",
      "103:\t61.5974\n",
      "104:\t61.9452\n",
      "105:\t61.4919\n",
      "106:\t60.9783\n",
      "107:\t61.7843\n",
      "108:\t61.0969\n",
      "109:\t61.0218\n",
      "110:\t60.5964\n",
      "111:\t60.3889\n",
      "112:\t60.1375\n",
      "113:\t60.8245\n",
      "114:\t60.2964\n",
      "115:\t59.9016\n",
      "116:\t61.9142\n",
      "117:\t59.7343\n",
      "118:\t59.3237\n",
      "119:\t59.2848\n",
      "120:\t58.6806\n",
      "121:\t59.2665\n",
      "122:\t61.3044\n",
      "123:\t59.1778\n",
      "124:\t58.3772\n",
      "125:\t60.1582\n",
      "126:\t58.1712\n",
      "127:\t57.8059\n",
      "128:\t57.6589\n",
      "129:\t58.3212\n",
      "130:\t57.5266\n",
      "131:\t59.6995\n",
      "132:\t59.1214\n",
      "133:\t57.0587\n",
      "134:\t57.0726\n",
      "135:\t56.8992\n",
      "136:\t57.6421\n",
      "137:\t59.1953\n",
      "138:\t57.2286\n",
      "139:\t57.6264\n",
      "140:\t56.3131\n",
      "141:\t56.6590\n",
      "142:\t58.7169\n",
      "143:\t56.0525\n",
      "144:\t59.2236\n",
      "145:\t57.1719\n",
      "146:\t55.5135\n",
      "147:\t58.3319\n",
      "148:\t57.7001\n",
      "149:\t55.5352\n",
      "150:\t57.9220\n",
      "151:\t55.1738\n",
      "152:\t56.2717\n",
      "153:\t54.6931\n",
      "154:\t56.5507\n",
      "155:\t57.6391\n",
      "156:\t55.3763\n",
      "157:\t54.8432\n",
      "158:\t54.4854\n",
      "159:\t54.2995\n",
      "160:\t54.2852\n",
      "161:\t54.2754\n"
     ]
    }
   ],
   "source": [
    "print('Epoch\\tLoss')\n",
    "for i, loss in enumerate(optimizer.loss):\n",
    "    print('{}:\\t{:.4f}'.format(i, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict the target values for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = optimizer.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample\tTruth\tPredicted\n",
      "0:\t23.6\t24.779\n",
      "1:\t32.4\t29.238\n",
      "2:\t13.6\t21.967\n",
      "3:\t22.8\t23.176\n",
      "4:\t16.1\t19.448\n",
      "5:\t20.0\t23.648\n",
      "6:\t17.8\t23.215\n",
      "7:\t14.0\t22.113\n",
      "8:\t19.6\t20.577\n",
      "9:\t16.8\t22.983\n",
      "10:\t21.5\t26.168\n",
      "11:\t18.9\t24.201\n",
      "12:\t7.0\t6.978\n",
      "13:\t21.2\t23.433\n",
      "14:\t18.5\t23.675\n",
      "15:\t29.8\t20.396\n",
      "16:\t18.8\t23.103\n",
      "17:\t10.2\t15.357\n",
      "18:\t50.0\t29.486\n",
      "19:\t14.1\t19.580\n",
      "20:\t25.2\t24.362\n",
      "21:\t29.1\t25.412\n",
      "22:\t12.7\t22.304\n",
      "23:\t22.4\t25.079\n",
      "24:\t14.2\t17.582\n",
      "25:\t13.8\t17.530\n",
      "26:\t20.3\t22.797\n",
      "27:\t14.9\t11.122\n",
      "28:\t21.7\t26.845\n",
      "29:\t18.3\t22.458\n",
      "30:\t23.1\t22.958\n",
      "31:\t23.8\t24.732\n",
      "32:\t15.0\t21.563\n",
      "33:\t20.8\t18.654\n",
      "34:\t19.1\t17.632\n",
      "35:\t19.4\t18.569\n",
      "36:\t34.7\t27.055\n",
      "37:\t19.5\t23.744\n",
      "38:\t24.4\t26.094\n",
      "39:\t23.4\t22.858\n",
      "40:\t19.7\t21.068\n",
      "41:\t28.2\t26.424\n",
      "42:\t50.0\t30.008\n",
      "43:\t17.4\t23.265\n",
      "44:\t22.6\t24.922\n",
      "45:\t15.1\t18.807\n",
      "46:\t13.1\t22.646\n",
      "47:\t24.2\t23.392\n",
      "48:\t19.9\t19.812\n",
      "49:\t24.0\t26.300\n",
      "50:\t18.9\t24.696\n",
      "51:\t35.4\t26.066\n",
      "52:\t15.2\t24.143\n",
      "53:\t26.5\t25.155\n",
      "54:\t43.5\t26.338\n",
      "55:\t21.2\t19.747\n",
      "56:\t18.4\t19.755\n",
      "57:\t28.5\t27.930\n",
      "58:\t23.9\t24.778\n",
      "59:\t18.5\t23.746\n",
      "60:\t25.0\t25.982\n",
      "61:\t35.4\t29.269\n",
      "62:\t31.5\t23.028\n",
      "63:\t20.2\t19.566\n",
      "64:\t24.1\t27.360\n",
      "65:\t20.0\t25.538\n",
      "66:\t13.1\t19.940\n",
      "67:\t24.8\t25.187\n",
      "68:\t30.8\t27.185\n",
      "69:\t12.7\t9.882\n",
      "70:\t20.0\t22.880\n",
      "71:\t23.7\t21.476\n",
      "72:\t10.8\t14.484\n",
      "73:\t20.6\t26.913\n",
      "74:\t20.8\t23.051\n",
      "75:\t5.0\t16.488\n",
      "76:\t20.1\t24.056\n",
      "77:\t48.5\t29.708\n",
      "78:\t10.9\t19.622\n",
      "79:\t7.0\t19.270\n",
      "80:\t20.9\t24.095\n",
      "81:\t17.2\t15.814\n",
      "82:\t20.9\t26.586\n",
      "83:\t9.7\t16.718\n",
      "84:\t19.4\t24.192\n",
      "85:\t29.0\t26.778\n",
      "86:\t16.4\t17.140\n",
      "87:\t25.0\t24.503\n",
      "88:\t25.0\t24.325\n",
      "89:\t17.1\t23.992\n",
      "90:\t23.2\t23.065\n",
      "91:\t10.4\t10.266\n",
      "92:\t19.6\t24.195\n",
      "93:\t17.2\t23.653\n",
      "94:\t27.5\t15.236\n",
      "95:\t23.0\t24.165\n",
      "96:\t50.0\t19.789\n",
      "97:\t17.9\t6.565\n",
      "98:\t9.6\t10.994\n",
      "99:\t17.2\t11.473\n",
      "100:\t22.5\t23.616\n",
      "101:\t21.4\t23.427\n",
      "102:\t12.0\t16.319\n",
      "103:\t19.9\t23.836\n",
      "104:\t19.4\t24.341\n",
      "105:\t13.4\t9.327\n",
      "106:\t18.2\t23.460\n",
      "107:\t24.6\t23.962\n",
      "108:\t21.1\t24.041\n",
      "109:\t24.7\t28.417\n",
      "110:\t8.7\t9.301\n",
      "111:\t27.5\t15.313\n",
      "112:\t20.7\t24.686\n",
      "113:\t36.2\t24.334\n",
      "114:\t31.6\t25.284\n",
      "115:\t11.7\t12.804\n",
      "116:\t39.8\t27.776\n",
      "117:\t13.9\t21.281\n",
      "118:\t21.8\t21.911\n",
      "119:\t23.7\t24.507\n",
      "120:\t17.6\t24.207\n",
      "121:\t24.4\t24.812\n",
      "122:\t8.8\t9.103\n",
      "123:\t19.2\t23.397\n",
      "124:\t25.3\t23.444\n",
      "125:\t20.4\t24.036\n",
      "126:\t23.1\t27.595\n",
      "127:\t37.9\t27.353\n",
      "128:\t15.6\t17.756\n",
      "129:\t45.4\t28.100\n",
      "130:\t15.7\t21.038\n",
      "131:\t22.6\t22.905\n",
      "132:\t14.5\t24.510\n",
      "133:\t18.7\t23.142\n",
      "134:\t17.8\t18.381\n",
      "135:\t16.1\t13.143\n",
      "136:\t20.6\t23.785\n",
      "137:\t31.6\t29.092\n",
      "138:\t29.1\t26.566\n",
      "139:\t15.6\t24.203\n",
      "140:\t17.5\t23.891\n",
      "141:\t22.5\t25.779\n",
      "142:\t19.4\t25.552\n",
      "143:\t19.3\t25.085\n",
      "144:\t8.5\t14.924\n",
      "145:\t20.6\t25.393\n",
      "146:\t17.0\t23.628\n",
      "147:\t17.1\t19.363\n",
      "148:\t14.5\t23.727\n",
      "149:\t50.0\t28.197\n",
      "150:\t14.3\t12.953\n",
      "151:\t12.6\t19.540\n"
     ]
    }
   ],
   "source": [
    "print('Sample\\tTruth\\tPredicted')\n",
    "for i in range(len(Y_test)):\n",
    "    print('{}:\\t{:.1f}\\t{:.3f}'.format(i, Y_test[i], Y_pred[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
